# V2V-Vulnerability-to-Vigilance: Fortifying DINO Models Against Adversarial Intrusions
### Authors : Ashay Ajbhani, Geeta Chandra Raju Bethala, Varuni Buereddy


<br>

Machine Learning classifiers, despite their sophistication, are
highly susceptible to adversarial attacks. We explore adver-
sarial attacks on machine learning models, emphasizing the
vulnerability of models to deliberate manipulations of in-
put data. Various adversarial attack algorithms, including the
Fast Gradient Sign Method (FGSM), Projected Gradient De-
scent (PGD), Transferable Adversarial Perturbations, DiffAttack are examined for their unique approaches
to deceiving models. The methodology covers experimental
setups, datasets, adversarial attacks (DiffAttack and PGD),
and defense strategies on DINO Representations. Evaluation results demon-
strate the impact of adversarial attacks on different models
and showcase a defense strategy, adversarial training the Dino Representations on the above attacks for improved ro-
bustness. Overall, the project contributes to the understanding of adversarial attacks and defense mechanisms in machine learning.

Dataset: Imagenet100

GPUs used for single-node and distributed-training : **NVIDIA RTX 8000, NVIDIA Tesla V100**.

# Adversarial Attack
```math
F_\theta(\text{Attack}(x ; G_\phi)) = F_\theta(x') \neq y
```
F is the target model, G is the surrogate model, x is the original data, x' is the adversarial image
## DiffAttack 

Here, we are analyzing the robustness of various neural network architectures against adversarial attacks, specifically using the DiffAttack method to generate adversarial examples. This approach is particularly effective because it intelligently modifies images in ways that are often imperceptible to humans but can significantly confuse AI models. Here we evaluate how different neural networks perform when confronted with these subtly altered images, compared to their performance with standard, unaltered images.

The log file I'm discussing presents a comparison of several architectures including ResNet, VGG, MobileNet, Inception, ConvNeXt, Vision Transformer (ViT), Swin Transformer, and various versions of Data-efficient Image Transformers (DeiT) and Mixers. Each model's accuracy is reported on both benign (normal) and adversarial examples. A consistent pattern emerged from our analysis: while all models exhibited high accuracy with benign examples, their performance drastically dropped when confronted with adversarial examples. For instance, ResNet showed a high accuracy of 97.23% on benign examples, but this plummeted to 3.77% on adversarial examples. This trend was observed across all models, although some, like ViT and DeiT-B, showed better resilience.

This significant drop in accuracy highlights the vulnerability of even advanced neural networks to adversarial attacks. Our findings underscore the importance of developing more robust AI systems that can withstand such manipulations. The fact that models like ViT and DeiT variants showed relatively better resistance suggests directions for future research. The use of DiffAttack for generating adversarial images proved effective in revealing these vulnerabilities, and it emphasizes the need for continuous improvement in both offensive and defensive strategies in AI.


![](images/n01537544_16_diff_resnet_image_ATKSuccess.png)

Original image "indigo_bunting" the accuracy is 98 % and the adversarial image only gives around 3 % accuracy on this class


# Transferability of DiffAttack on Other Models

The following table presents the performance of various neural network architectures against adversarial attacks using DiffAttack. The adversarial images were generated on the surrogate model (ViT) and then tested on different models to assess their resilience.

| Attack Model   | CA (%)   | AA (%)   |
| -------------- | -------- | -------- |
| **ViT (surrogate model)** | 84.1     | 9.4      |
| ResNet         | 80.6     | 39.06    |
| VGG            | 76.68    | 40.1     |
| MobileNet      | 74.5     | 38       |
| Inception      | 73.9     | 38.84    |

*Note:*
- *CA: Accuracy on Clean Images*
- *AA: Accuracy on Adversarial Images*



Adversarial Images Generated Using DiffAttack: [Link](https://drive.google.com/drive/folders/1_2S8KcZDavoir3M_b0lY7c95nvkO_wam?usp=drive_link )

## How to run?

## PGD Attack 
This function generates the PGD-Linf adversarial images 
```
def generate_attack(target_model, img, label):
    if args.pgd_attack == 'linf':
        adversary = LinfPGDAttack(target_model, loss_fn=None, eps=args.pgd_size, nb_iter=50, eps_iter=0.01, rand_init=True, clip_min=0.0, clip_max=1.0, targeted=False)
```

## Images Illustrating the Effect of Adversarial Attacks

The following images demonstrate the impact of adversarial attacks on neural network models. The first row shows original images, while the second row displays their adversarial counterparts.

| Original Images | Adversarial Images |
|:---------------:|:------------------:|
| ![Original Image 1](images/original_img_1-1.png) | ![Adversarial Image 1](images/adversarial_img_1-1.png) |
| ![Original Image 2](images/original_img_2-1.png) | ![Adversarial Image 2](images/adversarial_img_2-1.png) |
| ![Original Image 2](images/original_img_-1.png) | ![Adversarial Image 2](images/adversarial_img_-1.png) |

*Caption: The first row of images represents the original images. The second row displays the corresponding adversarial images, generated using the Projected Gradient Descent (PGD-Linf with Îµ = 0.03) attack on the Dino baseline model, which are misclassified when sent to a linear classifier.*


# Adversarial Training
## Dino Architecture 

![dkjesdbjef](images/dino.gif)


![](images/Rodino.png)

## How to run?

Our proposed implementati
on (both single node and distributed) are in the folder `D3GAN`. Run the following command for **single GPU** training: 

To train on the entire dataset,

```
python main_dino.py --arch vit_small --patch_size 16 --epochs 100 --batch_size_per_gpu 32 --data_path /data/imagenet-100/ --output_dir ./save/ --saveckp_freq 5
```

You can save the load the checkpoint of the Dino from [this link](https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain_full_checkpoint.pth) or you can alternatively train from scratch

To evaluate the defense model on the classifier,

```
python eval_linear.py --arch vit_small --patch_size 16 --epochs 100 --num_labels 100 --data_path /data/imagenet-100/ --output_dir ./save/ --linear_pretrained_weights ./checkpoint_Dino-reference_linear-010.pth.tar --evaluate
```


# Results and Observations

## Adversarial Attack


## Adversarial Defense


# References





