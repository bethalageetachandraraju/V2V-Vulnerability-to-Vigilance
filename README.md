# V2V-Vulnerability-to-Vigilance: Fortifying DINO Models Against Adversarial Intrusions
### Authors : Ashay Ajbhani, Geeta Chandra Raju Bethala, Varuni Buereddy


<br>

Machine Learning classifiers, despite their sophistication, are
highly susceptible to adversarial attacks. We explore adver-
sarial attacks on machine learning models, emphasizing the
vulnerability of models to deliberate manipulations of in-
put data. Various adversarial attack algorithms, including the
Fast Gradient Sign Method (FGSM), Projected Gradient De-
scent (PGD), Transferable Adversarial Perturbations, DiffAttack are examined for their unique approaches
to deceiving models. The methodology covers experimental
setups, datasets, adversarial attacks (DiffAttack and PGD),
and defense strategies on DINO Representations. Evaluation results demon-
strate the impact of adversarial attacks on different models
and showcase a defense strategy, adversarial training the Dino Representations on the above attacks for improved ro-
bustness. Overall, the project contributes to the understanding of adversarial attacks and defense mechanisms in machine learning.

Dataset: Imagenet100

GPUs used for single-node and distributed-training : **NVIDIA RTX 8000, NVIDIA Tesla V100**.

# Adversarial Attack
```math
F_\theta(\text{Attack}(x ; G_\phi)) = F_\theta(x') \neq y
```
F is the target model, G is the surrogate model, x is the original data, x' is the adversarial image
## DiffAttack 

Here, we are analyzing the robustness of various neural network architectures against adversarial attacks, specifically using the DiffAttack method to generate adversarial examples. This approach is particularly effective because it intelligently modifies images in ways that are often imperceptible to humans but can significantly confuse AI models. Here we evaluate how different neural networks perform when confronted with these subtly altered images, compared to their performance with standard, unaltered images.

The log file I'm discussing presents a comparison of several architectures including ResNet, VGG, MobileNet, Inception, ConvNeXt, Vision Transformer (ViT), Swin Transformer, and various versions of Data-efficient Image Transformers (DeiT) and Mixers. Each model's accuracy is reported on both benign (normal) and adversarial examples. A consistent pattern emerged from our analysis: while all models exhibited high accuracy with benign examples, their performance drastically dropped when confronted with adversarial examples. For instance, ResNet showed a high accuracy of 97.23% on benign examples, but this plummeted to 3.77% on adversarial examples. This trend was observed across all models, although some, like ViT and DeiT-B, showed better resilience.

This significant drop in accuracy highlights the vulnerability of even advanced neural networks to adversarial attacks. Our findings underscore the importance of developing more robust AI systems that can withstand such manipulations. The fact that models like ViT and DeiT variants showed relatively better resistance suggests directions for future research. The use of DiffAttack for generating adversarial images proved effective in revealing these vulnerabilities, and it emphasizes the need for continuous improvement in both offensive and defensive strategies in AI.


![](images/n01537544_16_diff_resnet_image_ATKSuccess.png)

Original image "indigo_bunting" the accuracy is 98 % and the adversarial image only gives around 3 % accuracy on this class


# Transferability of DiffAttack on Other Models

The following table presents the performance of various neural network architectures against adversarial attacks using DiffAttack. The adversarial images were generated on the surrogate model (ViT) and then tested on different models to assess their resilience.

| Attack Model   | CA (%)   | AA (%)   |
| -------------- | -------- | -------- |
| **ViT (surrogate model)** | 84.1     | 9.4      |
| ResNet         | 80.6     | 39.06    |
| VGG            | 76.68    | 40.1     |
| MobileNet      | 74.5     | 38       |
| Inception      | 73.9     | 38.84    |

*Note:*
- *CA: Accuracy on Clean Images*
- *AA: Accuracy on Adversarial Images*



Adversarial Images Generated Using DiffAttack: [Link](https://drive.google.com/drive/folders/1_2S8KcZDavoir3M_b0lY7c95nvkO_wam?usp=drive_link )

## How to run?

## PGD Attack 
This function generates the PGD-Linf adversarial images 
```
def generate_attack(target_model, img, label):
    if args.pgd_attack == 'linf':
        adversary = LinfPGDAttack(target_model, loss_fn=None, eps=args.pgd_size, nb_iter=50, eps_iter=0.01, rand_init=True, clip_min=0.0, clip_max=1.0, targeted=False)
```

## Images Illustrating the Effect of Adversarial Attacks

The following images demonstrate the impact of adversarial attacks on neural network models. The first row shows original images, while the second row displays their adversarial counterparts.

<table>
  <tr>
    <td>
      <img src="images/original_img_1-1.png" width="250px"/>
    </td>
    <td>
      <img src="images/adversarial_img_1-1.png" width="250px"/>
    </td>
  </tr>
  <tr>
    <td>
      <img src="images/original_img_2-1.png" width="250px"/>
    </td>
    <td>
      <img src="images/adversarial_img_2-1.png" width="250px"/>
    </td>
  </tr>
  <tr>
    <td>
      <img src="images/original_img_6-1.png" width="250px"/>
    </td>
    <td>
      <img src="images/adversarial_img_6-1.png" width="250px"/>
    </td>
  </tr>   
</table>

*Caption: The first row of images represents the original images. The second row displays the corresponding adversarial images, generated using the Projected Gradient Descent (PGD-Linf with ε = 0.03) attack on the Dino baseline model, which are misclassified when sent to a linear classifier.*



# Adversarial Training - Dino Architecture 

![dkjesdbjef](images/dino.gif)


![](images/Rodino.png)

## How to run?

Our proposed implementati
on (both single node and distributed) are in the folder `D3GAN`. Run the following command for **single GPU** training: 

To train on the entire dataset,

```
python main_dino.py --arch vit_small --patch_size 16 --epochs 100 --batch_size_per_gpu 32 --data_path /data/imagenet-100/ --output_dir ./save/ --saveckp_freq 5
```

You can save the load the checkpoint of the Dino from [this link](https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain_full_checkpoint.pth) or you can alternatively train from scratch

To evaluate the defense model on the classifier,

```
python eval_linear.py --arch vit_small --patch_size 16 --epochs 100 --num_labels 100 --data_path /data/imagenet-100/ --output_dir ./save/ --linear_pretrained_weights ./checkpoint_Dino-reference_linear-010.pth.tar --evaluate
```


# Results and Observations

## Adversarial Attack
In our experiments and training, we concentrated on the effectiveness of adversarial attacks, particularly utilizing the Projected Gradient Descent (PGD) method. The PGD attack, implemented with an ε value of 0.03 and conducted over 50 iterations, demonstrated a profound impact on the accuracy of neural network models. For example, the DINO model's performance drastically decreased under the PGD attack, with its clean accuracy falling from 86.94% to as low as 3.62% in different attack scenarios. This significant reduction in model accuracy across various perturbation levels illustrates the potency of PGD attacks.

Initially, we used DiffAttack but due to time constraints, the focus remained predominantly on the outcomes of the PGD method. The results highlight the vulnerabilities of advanced neural networks to structured adversarial attacks.


## Adversarial Defense

The table below compares the performance of the DINO model before and after adversarial training (AT). It highlights the model's Clean Accuracy (CA) and its robustness against \( \ell_{\infty} \)-norm Projected Gradient Descent (PGD) attacks with varying perturbation limits (ε).

| Embedding      | CA (%) | $\( \ell_{\infty} \)-PGD \( \epsilon = 0.01 \)$ | $\( \epsilon = 0.02 \)$ | $\( \epsilon = 0.03 \)$ |
| -------------- | ------ | -------------------------------------------- | --------------------- | --------------------- |
| DINO before AT | 86.94  | 28.00                                        | 10.02                 | 3.62                  |
| DINO after AT  | 82.60  | 50.12                                        | 32.26                 | 17.08                 |

*Table Caption: CA - Clean Accuracy, AT - Adversarial Training. This table showcases the natural accuracy score and the robustness of the DINO model against \( \ell_{\infty} \)-norm PGD attacks with different perturbation limits.*


Our defense strategy for the Dino model against adversarial threats involved integrating adversarially images into the training pipeline. We utilized the vit-small architecture within the DINO framework, focusing on tailored image augmentations for both the student and teacher models. This approach was designed to make adversarial and original image representations indistinguishable, thereby enhancing the model's resilience.

The defense effectiveness was evaluated using a custom classifier, named DinoPlusClassifier. This classifier combines the features extracted by the adversarially trained DINO model with a linear classification layer. The training, conducted over 10 epochs, yielded a model that was significantly more robust to adversarial attacks, as evidenced by its improved performance in classifying previously misclassified adversarial images.This table showcases not only an increase in the model's resilience against adversarial attacks but also maintains a reasonable balance in natural classification accuracy, highlighting the success of our defense training approach.


## References

1. Byun, J.; Go, H.; Cho, S.; and Kim, C. 2022. Exploiting Doubly Adversarial Examples for Improving Adversarial Robustness. In 2022 IEEE International Conference on Image Processing (ICIP), 1331–1335. IEEE.
2. Caron, M.; Touvron, H.; Misra, I.; Jégou, H.; Mairal, J.; Bojanowski, P.; and Joulin, A. 2021. Emerging Properties in Self-Supervised Vision Transformers. In Proceedings of the International Conference on Computer Vision (ICCV).
3. Chen, J.; Chen, H.; Chen, K.; Zhang, Y.; Zou, Z.; and Shi, Z. 2023. Diffusion Models for Imperceptible and Transferable Adversarial Attack. arXiv preprint arXiv:2305.08192.
4. Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.
5. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu, A. 2017. Towards deep learning models resistant to adversarial attacks. stat, 1050: 9.
6. Tsipras, D.; Santurkar, S.; Engstrom, L.; Turner, A.; and Madry, A. 2018. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152.
